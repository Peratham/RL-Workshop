{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    f = len(frames)\n",
    "    a = animation.FuncAnimation(plt.gcf(), animate, frames=f, interval=17)\n",
    "    display.display(display_animation(a, default_mode='once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Environment Parameters\n",
    "\"\"\"\n",
    "observation_space = 4\n",
    "action_space = 2\n",
    "env_name = 'CartPole-v0'\n",
    "seed = 12\n",
    "\n",
    "\"\"\"\n",
    "Agent Parameters\n",
    "\"\"\"\n",
    "episodes = 1000\n",
    "epsilon_init = 1.0\n",
    "epsilon_decay = 0.002\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "\"\"\"\n",
    "Model Parameters\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "hidden_dims = 16\n",
    "activation = 'relu'\n",
    "optim = 'rmsprop'\n",
    "loss_func = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cartpole_env = gym.make(env_name)\n",
    "cartpole_env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dqn():\n",
    "    dqn = Sequential()\n",
    "    dqn.add(Dense(hidden_dims, input_shape=(observation_space,)))\n",
    "    dqn.add(Activation(activation))\n",
    "    dqn.add(Dense(hidden_dims))\n",
    "    dqn.add(Activation(activation))\n",
    "    dqn.add(Dense(action_space))\n",
    "    dqn.compile(optimizer=optim, loss=loss_func)\n",
    "    return dqn\n",
    "\n",
    "simple_dqn = build_dqn()\n",
    "simple_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(model, buffer):\n",
    "    batch = random.sample(buffer, batch_size)\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in batch:\n",
    "        s, a, r, s_p, d = sample\n",
    "        inputs.append(s)\n",
    "        label = model.predict(s)\n",
    "        label[0][a] = r\n",
    "        if not d:\n",
    "            label[0][a] += gamma*np.max(model.predict(s_p))\n",
    "        labels.append(label)\n",
    "    inputs = np.squeeze(np.array(inputs), axis=1)\n",
    "    labels = np.squeeze(np.array(labels), axis=1)\n",
    "    model.fit(inputs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(env, model, buffer, epsilon=0.0,\n",
    "                training=False, render=False):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = np.expand_dims(env.reset(), 0)\n",
    "    frames = []\n",
    "    while not done:\n",
    "        if render:\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "        if training and random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        total_reward += reward\n",
    "        if training:\n",
    "            exp_tuple = (state, action, reward, next_state, done)\n",
    "            buffer.append(exp_tuple)\n",
    "        state = next_state\n",
    "    if training:\n",
    "        train_dqn(model, buffer)\n",
    "    epsilon -= epsilon_decay\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "    if render:\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        display_frames_as_gif(frames)\n",
    "    return total_reward, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_episode(cartpole_env, simple_dqn, simple_buffer, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def warmup_buffer(env, buffer):\n",
    "    for _ in range(50):\n",
    "        done = False\n",
    "        state = np.expand_dims(env.reset(), 0)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            exp_tuple = (state, action, reward, next_state, done)\n",
    "            buffer.append(exp_tuple)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warmup_buffer(cartpole_env, simple_buffer)\n",
    "eps = epsilon_init\n",
    "sum_reward = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    r, eps = run_episode(cartpole_env, simple_dqn, simple_buffer,\n",
    "                         epsilon=eps, training=True, render=False)\n",
    "    sum_reward += r\n",
    "    if episode % 10 == 0:\n",
    "        avg = sum_reward / 10\n",
    "        sum_reward = 0\n",
    "        start = episode - 9\n",
    "        print(\"Average total reward for episode \" + \\\n",
    "              f\"{start} through {episode}: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_episode(cartpole_env, simple_dqn, simple_buffer, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target Network Parameters\n",
    "\"\"\"\n",
    "target_decay = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "better_dqn = build_dqn()\n",
    "target_net = build_dqn()\n",
    "\n",
    "simple_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target(model, target):\n",
    "    model_weights = model.get_weights()\n",
    "    target_weights = target.get_weights()\n",
    "    for i in range(len(target_weights)):\n",
    "        target_weights[i] = target_decay * target_weights[i] + \\\n",
    "                            (1 - target_decay) * model_weights[i]\n",
    "    target.set_weights(target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, model, target, buffer, epsilon=0.0,\n",
    "                training=False, render=False):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = np.expand_dims(env.reset(), 0)\n",
    "    frames = []\n",
    "    while not done:\n",
    "        if render:\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "        if training and random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        total_reward += reward\n",
    "        if training:\n",
    "            exp_tuple = (state, action, reward, next_state, done)\n",
    "            buffer.append(exp_tuple)\n",
    "            if len(buffer) > 10000:\n",
    "                buffer.pop(0)\n",
    "        state = next_state\n",
    "    if training:\n",
    "        train_dqn(model, target, buffer)\n",
    "        update_target(model, target)\n",
    "    epsilon -= epsilon_decay\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "    if render:\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        display_frames_as_gif(frames)\n",
    "    return total_reward, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(model, target, buffer):\n",
    "    batch = random.sample(buffer, batch_size)\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in batch:\n",
    "        s, a, r, s_p, d = sample\n",
    "        inputs.append(s)\n",
    "        label = model.predict(s)\n",
    "        label[0][a] = r\n",
    "        if not d:\n",
    "            label[0][a] += gamma*np.max(target.predict(s_p))\n",
    "        labels.append(label)\n",
    "    inputs = np.squeeze(np.array(inputs), axis=1)\n",
    "    labels = np.squeeze(np.array(labels), axis=1)\n",
    "    model.fit(inputs, labels, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warmup_buffer(cartpole_env, simple_buffer)\n",
    "eps = epsilon_init\n",
    "sum_reward = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    r, eps = run_episode(cartpole_env, better_dqn, target_net, simple_buffer,\n",
    "                         epsilon=eps, training=True, render=False)\n",
    "    sum_reward += r\n",
    "    if episode % 10 == 0:\n",
    "        avg = sum_reward / 10\n",
    "        sum_reward = 0\n",
    "        start = episode - 9\n",
    "        print(\"Average total reward for episode \" + \\\n",
    "              f\"{start} through {episode}: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_episode(cartpole_env, better_dqn, target_net, simple_buffer, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "better_dqn.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(simple_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
